---
title: First Steps
description: Running the benchmark and testing your own agent.
---

## Prerequisites

Before running benchmarks, ensure you have:

1. [Installed the CLI](/cuabench/guide/getting-started/installation)
2. Created at least one [base image](/cuabench/guide/getting-started/images):

```bash
# Quick setup (Linux container, no KVM required)
cb image create linux-docker

# Check available images
cb image list
```

## Run a Basic Benchmark

The Cua-Bench CLI can run many different computer-use benchmarks and versions of benchmarks.

To see the available datasets, run `cb dataset list` or visit the [registry](http://cuabench.ai/cuabench/registry/).

### Step 1: Explore Interactively

Start by exploring a single task interactively to see how it works in a browser window:

```bash
cb interact click-icon --dataset cua-bench-basic
```

### Step 2: Validate Oracle Solutions

Next, verify your environment setup by running the oracle solutions:

```bash
cb run --dataset cua-bench-basic --oracle
```

### Step 3: Run with an Agent

Finally, perform a benchmark using the Cua Agent SDK. Tasks run **asynchronously by default** and return immediately with a run ID:

```bash
export ANTHROPIC_API_KEY=sk-....

# Run a single task (async - returns immediately)
cb run task tasks/click_button --agent cua-agent --model anthropic/claude-haiku-4-5

# Monitor progress in real-time
cb run watch <run_id>

# Or wait for completion inline
cb run task tasks/click_button --agent cua-agent --model anthropic/claude-haiku-4-5 --wait

# Run entire dataset in parallel
cb run dataset datasets/cua-bench-basic \
    --agent cua-agent \
    --model anthropic/claude-haiku-4-5 \
    --max-parallel 8 \
    --max-steps 10 \
    --max-variants 1
```

Results are saved to `~/.local/share/cua-bench/runs/<run_id>/` by default.

You should see output similar to:

```

    ⠀⣀⣀⡀⠀⠀⠀⠀⢀⣀⣀⣀⡀⠘⠋⢉⠙⣷⠀⠀ ⠀ 
 ⠀⠀⢀⣴⣿⡿⠋⣉⠁⣠⣾⣿⣿⣿⣿⡿⠿⣦⡈⠀⣿⡇⠃⠀
 ⠀⠀⠀⣽⣿⣧⠀⠃⢰⣿⣿⡏⠙⣿⠿⢧⣀⣼⣷⠀⡿⠃⠀⠀
 ⠀⠀⠀⠉⣿⣿⣦⠀⢿⣿⣿⣷⣾⡏⠀⠀⢹⣿⣿⠀⠀⠀⠀⠀⠀
 ⠀⠀⠀⠀⠀⠉⠛⠁⠈⠿⣿⣿⣿⣷⣄⣠⡼⠟⠁⠀cua-bench==v0.1.0
           toolkit for computer-use RL environments and benchmarks

████████████████████ 13/13
Model: anthropic/claude-haiku-4-5
Agent: cua-agent
Run ID: run-2768db7f



SESSION ID                                   ENVIRONMENT       VARIANT  STATUS           REWARD     
----------------------------------------     ---------------   -------  ---------------  ---------- 
task-baecb22e                                typing-input      0        ✗ failed         0.0        
task-05a6e068                                video-player      0        ✓ completed      1.0        
task-a8faeb43                                spreadsheet-cell  0        ✗ failed         0.0        
task-5a5f30c5                                toggle-switch     0        ✓ completed      1.0        
task-3555ee20                                select-dropdown   0        ✗ failed         0.0        
task-e9560151                                click-button      0        ✓ completed      1.0        
task-7ec326f2                                fill-form         0        ✗ failed         0.0        
task-57dae23d                                date-picker       0        ✗ failed         0.0        
task-01276975                                drag-slider       0        ✗ failed         0.0        
task-4b875cd2                                color-picker      0        ✓ completed      1.0        
task-c933d5b0                                drag-drop         0        ✗ failed         0.0        
task-ec44d643                                right-click-menu  0        ✗ failed         0.0        
task-7c4acb4c                                click-icon        0        ✓ completed      1.0        



✓ All sessions completed!
Average Reward: 0.385

REWARD  COUNT
-------------
1.0     5
0.0     8
```

You can see an overview table of your past runs with `cb run list`

## Run Windows Arena

Windows Arena is a benchmark with 173 tasks across 12 Windows application domains.

<Callout title="Requirements" type="info">
  Windows Arena requires x86_64 Linux with KVM support (nested virtualization). Setup takes ~1-2 hours.
</Callout>

### Step 1: Create Windows Base Image

```bash
# Download Windows ISO and create base image (~1-2 hours)
cb image create windows-qemu --download-iso

# With WinArena benchmark apps pre-installed (recommended)
cb image create windows-qemu --download-iso --winarena-apps

# Monitor progress (VNC at http://localhost:8006)
docker logs -f cua-setup-windows
```

### Step 2: Verify Image (Optional)

```bash
# Start interactive shell to verify the image works
cb image shell windows-qemu

# Access VNC at http://localhost:8006
# Press Ctrl+C to stop when done verifying
```

### Step 3: Run Tasks

```bash
# Run specific task with oracle
cb interact tasks/winarena_adapter --variant-id 0 --oracle

# Run with agent (2-container architecture)
cb run task tasks/winarena_adapter \
    --agent cua-agent \
    --model anthropic/claude-sonnet-4-20250514 \
    --image windows-qemu

# Run entire dataset in parallel
cb run dataset datasets/winarena \
    --agent cua-agent \
    --model anthropic/claude-sonnet-4-20250514 \
    --image windows-qemu \
    --max-parallel 4
```

## View Past Runs

```bash
# List all runs with statistics
cb run list

# Watch a run in real-time
cb run watch <run_id>

# Check status of a specific run
cb run status <run_id>

# View logs
cb run logs <run_id>

# View traces from a specific run
cb trace grid ~/.local/share/cua-bench/runs/<run_id>/
```

## Export Training Data

Export agent trajectories to training datasets:

```bash
# Export with aguvis-stage-1 format
cb dataset build outputs/<run-id> --save-dir datasets/

# Export with gui-r1 format
cb dataset build outputs/<run-id> --mode gui-r1 --save-dir datasets/

# Push to Hugging Face
cb dataset build outputs/<run-id> --push-to-hub --repo-id username/my-dataset
```