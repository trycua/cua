---
title: Supported Model Providers
---

## Supported Models

### Cua VLM Router (Recommended)

Use Cua's cloud inference API for intelligent routing and cost optimization with a single API key. Cua manages all provider infrastructure and credentials for you.

```python
# Anthropic Claude models
model="cua/anthropic/claude-sonnet-4.5"   # Claude Sonnet 4.5 (recommended)
model="cua/anthropic/claude-haiku-4.5"    # Claude Haiku 4.5 (faster)

# Google Gemini models
model="cua/google/gemini-3-pro-preview"   # Gemini 3 Pro Preview (most powerful)
model="cua/google/gemini-3-flash-preview" # Gemini 3 Flash Preview (fastest and cheapest, recommended for balance)
```

**Benefits:**

- Single API key for multiple providers
- Cost tracking and optimization
- Fully managed infrastructure (no provider keys to manage)

[Learn more about Cua VLM Router â†’](/agent/supported-model-providers/cua-vlm-router)

---

### Anthropic Claude (Computer Use API - BYOK)

Access Anthropic's computer use models directly or through Azure AI Foundry using your own API key (BYOK).

#### Via Anthropic API

```python
model="anthropic/claude-haiku-4-5-20251001"   # Claude Haiku 4.5 (fastest, cost-effective)
model="anthropic/claude-sonnet-4-5-20250929"  # Claude Sonnet 4.5 (recommended)
model="anthropic/claude-opus-4-5-20251101"    # Claude Opus 4.5 (most advanced)
```

**Setup:** Set `ANTHROPIC_API_KEY` environment variable with your Anthropic API key.

#### Via Azure AI Foundry

```python
model="anthropic/claude-haiku-4-5"
model="anthropic/claude-sonnet-4-5"
model="anthropic/claude-opus-4-5"
```

**Setup:**

- Set `ANTHROPIC_API_KEY` environment variable with your Azure AI Foundry key
- Set `ANTHROPIC_API_BASE` to your Azure endpoint (e.g., `https://<your-resource>.services.ai.azure.com/anthropic`)

### OpenAI Computer Use Preview (BYOK)

Direct access to OpenAI's computer use models using your own OpenAI API key (BYOK).

```python
model="openai/computer-use-preview"
```

**Setup:** Set `OPENAI_API_KEY` environment variable with your OpenAI API key.

### Google Gemini (BYOK)

Access Google's Gemini models with native computer use capabilities using your own credentials (BYOK).

```python
model="gemini-3-flash-preview"  # Gemini 3 Flash (fastest, cheapest)
model="gemini-2.5-computer-use-preview"    # Computer Use Specific Gemini and general computer use
model="gemini-3-pro-preview"  # Gemini 3 Pro (most advanced)
```

**Setup Options:**

#### Option 1: Google AI Studio (Recommended for quick start)

Set the `GOOGLE_API_KEY` environment variable with your Google AI Studio API key.

```bash
export GOOGLE_API_KEY="your-google-ai-studio-key"
```

#### Option 2: Google Cloud Vertex AI

For production deployments or enterprise use, configure Vertex AI credentials:

```bash
export GOOGLE_CLOUD_PROJECT="your-gcp-project-id"
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
export GOOGLE_CLOUD_LOCATION="us-central1"  # or your preferred region
```

**Note:** When `GOOGLE_API_KEY` is set, it takes precedence. If not set, the SDK will automatically use Vertex AI with the configured Google Cloud credentials.

### UI-TARS (Local or Huggingface Inference)

Run UI-TARS models locally for privacy and offline use.

```python
model="huggingface-local/ByteDance-Seed/UI-TARS-1.5-7B"
model="ollama_chat/0000/ui-tars-1.5-7b"
```

### Omniparser + Any LLM

Combine Omniparser for UI understanding with any LLM provider.

```python
model="omniparser+ollama_chat/mistral-small3.2"
model="omniparser+vertex_ai/gemini-pro"
model="omniparser+anthropic/claude-sonnet-4-5-20250929"
model="omniparser+openai/gpt-4o"
```
