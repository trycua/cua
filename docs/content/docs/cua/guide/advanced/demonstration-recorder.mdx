---
title: Human Demonstration Recorder
description: Record human demonstrations from CUA sandboxes for training and evaluation
---

CUA sandboxes include a [noVNC fork](https://github.com/trycua/novnc) with built-in recording capabilities. This lets you capture human demonstrations as replayable session files for training data collection, evaluation baselines, or debugging.

<video controls width="100%">
  <source src="/docs/videos/demonstration-recorder.mp4" type="video/mp4" />
</video>

## Manual Recording

### 1. Start a sandbox and open the VNC UI

Start a sandbox using [Docker](/docs/cua/guide/get-started/set-up-sandbox) or [Cua Cloud](/docs/cua/guide/get-started/set-up-sandbox). The VNC UI will be available at:

- **Docker**: `http://localhost:8006`
- **Cloud**: `https://{sandbox-name}.sandbox.cua.ai/vnc.html`

### 2. Enable recording

Add `autorecord=1` to the VNC URL to automatically start recording when you connect:

```
http://localhost:8006/vnc.html?autoconnect=1&autorecord=1
```

Or click the **record button** in the control bar before connecting.

### 3. Perform your demonstration

Interact with the sandbox normally. All VNC traffic (screen updates and input events) is captured.

### 4. Download the recording

Click **Stop Recording**, then **Download** to save the session as a `.js` file.

### 5. Replay the recording

Start the playback UI:

```bash
# Clone the noVNC fork
git clone https://github.com/trycua/novnc
cd novnc

# Move your recording into the recordings folder
mv ~/Downloads/vnc-recording-*.js ./recordings/

# Start the local server
./utils/novnc_proxy
```

Then open the playback URL:

```
http://localhost:6080/tests/vnc_playback.html?data=../recordings/your-recording.js
```

## Streaming to External Server

For automated pipelines, stream recordings directly to a custom server using the `record_url` parameter. Here's a simple recording server:

```python
import asyncio
import websockets

async def handle_recording(websocket):
    with open("recording.bin", "wb") as f:
        async for message in websocket:
            f.write(message)
    print("Recording saved")

async def main():
    async with websockets.serve(handle_recording, "localhost", 6090):
        print("Recording server running on ws://localhost:6090")
        await asyncio.Future()

asyncio.run(main())
```

Then connect with streaming enabled:

```
http://localhost:8006/vnc.html?autoconnect=1&autorecord=1&record_url=ws://localhost:6090
```

Recording data streams to disk in real-time via WebSocket, so the file is ready immediately when the session ends.

## URL Parameters

| Parameter             | Description                                    |
| --------------------- | ---------------------------------------------- |
| `autorecord=1`        | Start recording when connection is established |
| `record_url=ws://...` | Stream recording to external WebSocket server  |

## Processing Demonstrations

Recordings are raw noVNC data streams containing VNC protocol messages. The CUA agent SDK includes a processor that converts these into semantic action traces suitable for agent prompting.

### Parse a Recording

```bash
# Basic parsing - extracts events and generates captions
python -m agent.process_demonstration vnc-recording-2026-01-22.js

# Add task context
python -m agent.process_demonstration vnc-recording-2026-01-22.js \
  --task "Login to email and compose a new message" \
  --output email_workflow.json

# Use OpenAI instead of Anthropic for captions
python -m agent.process_demonstration recording.js \
  --provider openai \
  --model gpt-4o
```

### Output Format

The processor outputs a JSON file with the following structure:

```json
{
  "trajectory": [
    {
      "step_idx": 1,
      "screenshot": "base64...",
      "cropped_screenshot": "base64...",
      "caption": {
        "observation": "The screen shows a login form with email and password fields",
        "think": "User intends to enter credentials to access the application",
        "action": "Click on the email input field",
        "expectation": "Field becomes focused and ready for text input"
      },
      "raw_event": {
        "timestamp": 1234,
        "type": "click",
        "data": { "x": 450, "y": 320, "button": "left" }
      }
    }
  ],
  "skill_prompt": "You have been shown a demonstration of how to perform this task...",
  "metadata": {
    "recording_file": "vnc-recording-2026-01-22.js",
    "task_description": "Login to email",
    "total_steps": 12
  }
}
```

For more details on prompting agents with demonstrations, see [Demonstration-Guided Agents](/docs/cua/guide/advanced/demonstration-guided-agents).

## Reference

For related capabilities, see:

- [ShowUI-Aloha](https://github.com/showlab/ShowUI-Aloha) - Human-taught GUI agent that learns from demonstrations
