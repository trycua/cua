# A Story of Computer-Use: Where We Started, Where We're Headed

_Published on February 5, 2026 by the Cua Team_

Last week, Clawdbot went from a niche open-source project to 80,000 GitHub stars. TechCrunch wrote about it. Everyone's suddenly talking about "computer-use agents."

<div align="center">
  <img src="./assets/story_1.jpeg" alt="Clawdbot Star History" width="600" />
</div>

But this didn't happen overnight. The idea of AI agents that can see your screen and click around has been brewing for almost a decade. Here's the timeline of how we got here—and why it's all converging now.

## 2016: The Seed

OpenAI released Universe—a platform for training AI on games and websites. The pitch: measure intelligence by how well AI can navigate the same digital environments humans use.

<div align="center">
  <img src="./assets/story_2.jpeg" alt="OpenAI Universe" width="600" />
</div>

It was too early. The models weren't there. But the idea was planted: **pixels in, actions out**.

## 2023: Vision Arrives

Everything changed when GPT-4V dropped. Suddenly we had models that could actually _see_.

<div align="center">
  <img src="./assets/story_3.jpeg" alt="GPT-4V System Card" width="600" />
</div>

Andrej Karpathy started riffing on "LLM OS"—the idea that language models could become the CPU of a new kind of computer, with peripherals like video, audio, browsers, and file systems all connected.

<div align="center">
  <img src="./assets/story_4.jpeg" alt="Karpathy LLM OS" width="600" />
</div>

People started building. GPT-4V-Act appeared on Reddit—a proof-of-concept browser agent that could actually navigate websites.

<div align="center">
  <img src="./assets/story_5.jpeg" alt="GPT-4V-Act Reddit" width="600" />
</div>

Microsoft started talking about "The AI PC"—local agents running on your machine, controlling your computer.

<div align="center">
  <img src="./assets/story_6.jpeg" alt="Microsoft UFO - The AI PC" width="600" />
</div>

## The Gap

But there was a problem. The benchmark numbers were brutal:

<div align="center">
  <img src="./assets/story_7.png" alt="Human vs AI Success Rate" width="400" />
</div>

Humans: 72%. Best AI: ~12%.

The models could see, but they couldn't act reliably. Every demo looked impressive until you tried it yourself.

## Late 2024: The Race Begins

Microsoft released OmniParser—a way to convert screenshots into structured elements that models could reason about.

<div align="center">
  <img src="./assets/story_8.jpeg" alt="OmniParser" width="600" />
</div>

Then Anthropic made their move. October 22, 2024: Claude Computer Use.

<div align="center">
  <img src="./assets/story_9.jpeg" alt="Claude Computer Use" width="600" />
</div>

Ethan Mollick wrote about it immediately: "When you give a Claude a mouse."

<div align="center">
  <img src="./assets/story_10.jpeg" alt="Ethan Mollick - When you give a Claude a mouse" width="600" />
</div>

A week later, Browser-Use hit Hacker News—open-source browser automation that anyone could run. 77K stars eventually.

<div align="center">
  <img src="./assets/story_11.jpeg" alt="Browser-Use Show HN" width="600" />
</div>

## 2025: Mainstream Adoption

January 2025: OpenAI launched Operator. Computer-use for the masses.

<div align="center">
  <img src="./assets/story_12.jpeg" alt="OpenAI Operator" width="600" />
</div>

The benchmarks started improving:

<div align="center">
  <img src="./assets/story_13.png" alt="OpenAI CUA vs Claude benchmarks" width="400" />
</div>

March 2025: Manus AI came out of nowhere with a 2 million person waitlist. The "general AI agent" promise was resonating.

<div align="center">
  <img src="./assets/story_14.jpeg" alt="Manus AI" width="600" />
</div>

Then CoAct-1 hit 60.76% on OSWorld—finally closing the gap with humans.

<div align="center">
  <img src="./assets/story_15.png" alt="CoAct-1 Performance" width="400" />
</div>

## The CLI Insight

December 2025: Geohot wrote something that reframed everything.

<div align="center">
  <img src="./assets/story_16.jpeg" alt="Geohot - Computer Use Models" width="600" />
</div>

"Turns out the idea wasn't a desktop emulator with a keyboard and mouse, it was just a command line."

Claude Code was crushing it—not by clicking pixels, but by running programs. The insight spread. Guillermo Rauch put it plainly:

<div align="center">
  <img src="./assets/story_17.jpeg" alt="Guillermo Rauch on CLI" width="600" />
</div>

"Coding agents are, at their core, computer-use agents."

## January 2026: Clawdbot

And then Clawdbot happened.

<div align="center">
  <img src="./assets/story_18.jpeg" alt="TechCrunch - Clawdbot" width="600" />
</div>

A composable plugin paradigm. Personal AI assistant over messaging. Suddenly everyone wanted one.

80K stars in a week. TechCrunch coverage. The moment computer-use agents stopped being a research curiosity and became something people actually use.

## The Full Timeline

<div align="center">
  <img src="./assets/story_19.png" alt="Computer-Use Agent Timeline" width="700" />
</div>

Ten years from Universe to Clawdbot. The models got better. The benchmarks caught up. The paradigm shifted from "click pixels" to "run programs." And now we're here.

At Cua, we've been building the infrastructure for this moment—sandboxes, VMs, computer-servers. The plumbing that makes computer-use agents actually work.

The next chapter is being written right now.

---

- [Cua Repository](https://github.com/trycua/cua)
- [Clawdbot](https://github.com/clawdbot/clawdbot)
