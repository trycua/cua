# A Story of Computer-Use: Where We Started, Where We're Headed

_Published on February 5, 2026 by the Cua Team_

Last week, Clawdbot went from a niche open-source project to 80,000 GitHub stars. TechCrunch wrote about it. Everyone's suddenly talking about "computer-use agents."

![story_1](https://github.com/user-attachments/assets/dc4ce9bb-f5d9-41c4-9b86-ea669fd741b0)


But this didn't happen overnight. The idea of AI agents that can see your screen and click around has been brewing for almost a decade. Here's the timeline of how we got here—and why it's all converging now.

## 2016: The Seed

OpenAI released Universe—a platform for training AI on games and websites. The pitch: measure intelligence by how well AI can navigate the same digital environments humans use.

![story_2](https://github.com/user-attachments/assets/d6f55f96-b803-42c3-adf3-89f8335d7c59)


It was too early. The models weren't there. But the idea was planted: **pixels in, actions out**.

## 2023: Vision Arrives

Everything changed when GPT-4V dropped. Suddenly we had models that could actually _see_.

![story_3](https://github.com/user-attachments/assets/f72cbb25-7a04-45fe-b1ae-a22702df7f08)


Andrej Karpathy started riffing on "LLM OS"—the idea that language models could become the CPU of a new kind of computer, with peripherals like video, audio, browsers, and file systems all connected.

![story_4](https://github.com/user-attachments/assets/04e62dfd-5e33-4904-9b21-895f475247bb)


People started building. GPT-4V-Act appeared on Reddit—a proof-of-concept browser agent that could actually navigate websites.

![story_5](https://github.com/user-attachments/assets/b8b740fd-72cd-43f0-912d-44bd0dbabac7)


Microsoft started talking about "The AI PC"—local agents running on your machine, controlling your computer.

![story_6](https://github.com/user-attachments/assets/b488b367-c45d-48a0-9138-18924a0a7f85)

## The Gap

But there was a problem. The benchmark numbers were brutal:

<img width="678" height="104" alt="story_7" src="https://github.com/user-attachments/assets/551576be-2a19-45df-8878-11675cbcfcd8" />


Humans: 72%. Best AI: ~12%.

The models could see, but they couldn't act reliably. Every demo looked impressive until you tried it yourself.

## Late 2024: The Race Begins

Microsoft released OmniParser—a way to convert screenshots into structured elements that models could reason about.

![story_8](https://github.com/user-attachments/assets/cbbcf62b-b691-4f32-a86a-9de3cd78f355)

Then Anthropic made their move. October 22, 2024: Claude Computer Use.

![story_9](https://github.com/user-attachments/assets/19c39ddd-70c4-4385-bbdc-a9241d9d83b7)


Ethan Mollick wrote about it immediately: "When you give a Claude a mouse."

![story_10](https://github.com/user-attachments/assets/b7a02392-7196-464c-80d8-a63d8f3264c9)


A week later, Browser-Use hit Hacker News—open-source browser automation that anyone could run. 77K stars eventually.

![story_11](https://github.com/user-attachments/assets/46e5a8e2-cf16-45fa-a94d-1e0fd54f2b0c)


## 2025: Mainstream Adoption

January 2025: OpenAI launched Operator. Computer-use for the masses.

![story_12](https://github.com/user-attachments/assets/f81fc03a-f135-4a14-99c9-583d4c90e427)


The benchmarks started improving:

<img width="834" height="154" alt="story_13" src="https://github.com/user-attachments/assets/b07d54a6-cd71-480e-94ec-19d22013421c" />


March 2025: Manus AI came out of nowhere with a 2 million person waitlist. The "general AI agent" promise was resonating.

![story_14](https://github.com/user-attachments/assets/072782ca-238f-4032-9fc7-cb6c0bbc1446)


Then CoAct-1 hit 60.76% on OSWorld—finally closing the gap with humans.

<img width="822" height="146" alt="story_15" src="https://github.com/user-attachments/assets/5758cc37-0f3b-4352-9078-b01e125c0834" />


## The CLI Insight

December 2025: Geohot wrote something that reframed everything.

![story_16](https://github.com/user-attachments/assets/46af6e71-cade-40f1-a638-e05ced4f88a0)


"Turns out the idea wasn't a desktop emulator with a keyboard and mouse, it was just a command line."

Claude Code was crushing it—not by clicking pixels, but by running programs. The insight spread. Guillermo Rauch put it plainly:

![story_17](https://github.com/user-attachments/assets/4bfe32a0-1ff7-4520-ac8f-31004df7a4f4)


"Coding agents are, at their core, computer-use agents."

## January 2026: Clawdbot

And then Clawdbot happened.

![story_18](https://github.com/user-attachments/assets/8eda45bd-391f-4513-9ea0-a0bc3d121117)


A composable plugin paradigm. Personal AI assistant over messaging. Suddenly everyone wanted one.

80K stars in a week. TechCrunch coverage. The moment computer-use agents stopped being a research curiosity and became something people actually use.

## The Full Timeline

<img width="1200" height="768" alt="story_19" src="https://github.com/user-attachments/assets/f327ff4e-cbf7-482d-8b54-25d538778671" />


Ten years from Universe to Clawdbot. The models got better. The benchmarks caught up. The paradigm shifted from "click pixels" to "run programs." And now we're here.

At Cua, we've been building the infrastructure for this moment—sandboxes, VMs, computer-servers. The plumbing that makes computer-use agents actually work.

The next chapter is being written right now.

---

- [Cua Repository](https://github.com/trycua/cua)
- [Clawdbot](https://github.com/clawdbot/clawdbot)
